/**
 * @author - Paul Baird-Smith, February 2018.
 *
 * An n-gram represents a language model, that generates a sentence of text
 * according to conditional probabilities on previous word sequences. If we define
 * a string of words w_1^m as a string of m words, an n-gram computes and stores
 *
 *     P(w_m | w_{m-1}, ..., w_{m-n})
 *
 * in a conditional probability table. These values are computed using training data,
 * which in this case are fiction novels. In this implementation, our table does not
 * hold probabilities as floats, but instead holds conditional frequencies, which are
 * the number of occurences of a word following a sequence. These tables are essentially
 * isomorphic, and the conditional frequency tables can simply be thought of as
 * "unnormalized" analogues of conditional probability tables.
 *
 * This implementation of n-grams adds a new parameter, k, a distance, so that we can now
 * talk of (n,k)-grams. k represents the distance from the next predicted word to the
 * word sequence it is conditioned on, so that our analogous conditional probability
 * would resemble:
 *
 *     P(w_m | w_{m - 1 - k}, ..., w_{m - n - k})
 *
 * Several of these n-grams can be used jointly to best predict the next word. We hope
 * that the performance of these (n,k)-grams can be compared to the performances of 
 * (n+k)-grams, as the conditional frequency tables of the collection of (n,k)-grams 
 * should take exponentially less space to store than those of the (n+k)-grams (assuming
 * a large enough training set).
 *
 * @email - ppb366@cs.utexas.edu
 */


// Imports
import java.util.ArrayList;
import java.util.HashMap;
import java.util.Scanner;
import java.util.Random;

import java.util.NoSuchElementException;

import java.io.File;

public class NGram {
    
    /**
     * Constants for n in the n-gram and the distance to leave between the
     * next predicted word and the previous words to condition on.
     */
    private final int distance, depth;

    /**
     * Maintains the number of occurences of every word following every string
     * of previous words. Keyed by a String composed of depth - 1 words, and
     * values are HashMaps keyed by following words, with frequency counts as values
     */
    private HashMap<String, HashMap<String, Integer>> condFreqs;

    /**
     * An iterator over the words composing the training String
     */
    private Scanner txtScan;

    /**
     * Static and final Strings representing setence start and end tokens
     */
    public static final String START_TOKEN = "<s>";
    public static final String END_TOKEN = "</s>";

    /**
     * Random object used to select the next predicted word
     */
    private Random r;

    /**
     * Default n-gram is a bigram
     */
    private static final int DEFAULT_DEPTH = 2;

    /**
     * Default is to predict the next immediate word in the text
     */
    private static final int DEFAULT_DISTANCE = 1;

    /**
     * The maximum length of a sentence generated by the n-gram
     */
    private static final int MAX_SENT_LENGTH = 100;


    /**
     * Constructor taking the depth of the n-gram, and the distance
     *
     * @param depth - The n-gram conditions on the previous depth - 1 words
     * @param distance - The n-gram predicts the next word on the depth - 1
     * words starting from this distance back
     */
    public NGram(int depth, int distance) {
	this.depth = depth;
	this.distance = distance;
	initVars();
    }

    /**
     * Constructor taking just depth of n-gram
     *
     * @param depth - The n-gram conditions on the previous depth - 1 words
     */
    public NGram(int depth) {
	this.depth = depth;
	this.distance = NGram.DEFAULT_DISTANCE;
	initVars();
    }

    /**
     * Constructor for a default bigram
     */
    public NGram() {
	this.depth = NGram.DEFAULT_DEPTH;
	this.distance = NGram.DEFAULT_DISTANCE;;
	initVars();
    }

    /**
     * Initialize the necessary global variables
     */
    public void initVars() {
	this.r = new Random();
	this.condFreqs = new HashMap<String, HashMap<String, Integer>>();
	this.txtScan = null;
    }


    /**
     * Utility method that returns the HashMap maintaining the conditional
     * frequencies of the n-gram
     *
     * @return - The HashMap of conditional frequencies of the n-gram
     */
    public HashMap<String, HashMap<String, Integer>> getMap() {
	return this.condFreqs;
    }


    /**
     * Uses the passed training string to update the n-gram's frequency counts
     *
     * @param text - The training String used to update the conditional frequencies
     */
    public void learn(String text) {

	// Initialize the textScanner to iterate over the training String
	this.txtScan = new Scanner(text);
	
	// Maintain the number of words iterated over. This could be an important
	// metric
	int numWords = 0;

	// Maintain an array of the previous depth - 1 words, used to predict the
	// next word
	String[] prevWords = new String[this.depth - 1];
	String nextWord = "";
	
	try {
	    // Get the first depth - 1 words in the training String
	    for(int wordCount = 0; wordCount < prevWords.length; wordCount++) {
		prevWords[wordCount] = txtScan.next().toLowerCase();
		numWords++;
	    }

	    // Get the next word in the sequence
	    nextWord = txtScan.next().toLowerCase();
	} catch(NoSuchElementException ne) {
	    System.err.println("ERROR: Passed String is too short.");
	    return;
	} catch(IllegalStateException ie) {
	    System.err.println("ERROR: Scanner is closed.");
	    return;
	}

	// Nothing more to learn from this String. The edge case, where there is
	// exactly one conditional to learn, is passed over.
	if(!txtScan.hasNext()) {
	    return;
	}

	// Update the conditional frequencies, as long as we have new information
	// from the training String
	do {

	    // Generate a String from the previous words
	    String prev = "";
	    for(String word: prevWords) {
		prev += word + " ";
	    }
	    prev = prev.trim();

	    // Include prev String as a key in our conditional frequencies, if not
	    // already there
	    if(!condFreqs.containsKey(prev)) {
		condFreqs.put(prev, new HashMap<String, Integer>());
	    }

	    // Initialize a new HashMap if the word has not been seen before
	    HashMap<String, Integer> wordFreqs = condFreqs.get(prev);

	    // Update the HashMap of frequency counts by incrementing the count of
	    // the next word in the text
	    if(!wordFreqs.containsKey(nextWord)) {
		wordFreqs.put(nextWord, 1);
	    } else {
		int freq = wordFreqs.get(nextWord);
		wordFreqs.put(nextWord, freq + 1);
	    }

	    // Update number of words iterated through
	    numWords++;

	    // No need to reinitialize our array of previous words, just shift
	    // all the constituent words to the left by 1.
	    for(int index = 0; index < prevWords.length - 1; index++) {
		prevWords[index] = prevWords[index + 1];
	    }

	    // Our next word becomes part of our previous words array
	    prevWords[prevWords.length - 1] = nextWord;

	    // Grab the next word in the training String
	    nextWord = txtScan.next().toLowerCase();

	} while(txtScan.hasNext()); // If this condition is met, we have learned everything
    }

    /**
     * Called by generateText() method to generate a random word based on the weighted
     * conditional frequency table of prevWord
     *
     * @param prevWord - A String of words that are used to key into condFreqs. These 
     * are the words we condition on to predict.
     */
    public String predictWord(String prevWord) {

	// Get the frequency table for the next word following the passed String
	HashMap<String, Integer> wordMap = condFreqs.get(prevWord);
	if(wordMap == null) {
	    System.out.println("ERROR: Could not find word map for word: " + prevWord);
	    return null;
	}

	// Find the sum of all the frequencies (this amounts to finding how many times
	// the String occured in the training String)
	int sum = 0;
	for(String key: wordMap.keySet()) {
	    sum += wordMap.get(key);
	}

	// Find a random number from 0 to (sum-1), used to select a random value from
	// the wordMap
	int randIndex = this.r.nextInt(sum);

	// Select a random next word from the weighted wordMap
	int curSum = 0;
	for(String key: wordMap.keySet()) {
	    if((curSum += wordMap.get(key)) >= randIndex) {
		return key;
	    }
	}
	
	// No word was found. Notify user of error.
	System.err.println("ERROR: No next word selected.");
	return null;
    }


    /**
     * Generates a sentence of text, using the n-gram's conditional frequencies
     *
     * @return - A sentence "in the style of" the training data.
     */
    public String generateText() {

	// Maintain a StringBuffer for generating the sentence, and hold a sequence
	// of previous words, as well as an array of these words.
	StringBuffer sb = new StringBuffer();
	String[] prev = new String[this.depth - 1];
	String prevSeq = "";

	// Find a start to a sentence somewhere in the trained conditional frequency
	// table, using a randomized algorithm (not the fastest, probably)
	while(!prevSeq.startsWith("<s>")) {
	    int randIndex = this.r.nextInt(condFreqs.size());
	    int count = 0;	
	    for(String key: condFreqs.keySet()) {
		if (count >= randIndex) {
		    prevSeq = key;
		    prev = key.split(" ");
		    break;
		}
		count++;
	    }
	}

	// Predict a next word, and append the initial sequence of words to sb
	String nextWord = predictWord(prevSeq);
	sb.append(" " + prevSeq);

	// Maintain the number of words to avoid an overflow (we limit our sentence
	// length to 100 words)
	int numWords = 0;

	// Continually predict a new word, until we reach the end of a sentence
	while(!nextWord.equals(NGram.END_TOKEN) && numWords < NGram.MAX_SENT_LENGTH) {

	    // Add the next word to the sentence
	    sb.append(" " + nextWord);

	    // Update the prev array, and the prevSeq String
	    prevSeq = "";
	    for(int index = 0; index < prev.length - 1; index++) {
		prevSeq += prev[index + 1] + " ";
		prev[index] = prev[index + 1];
	    }
	    prevSeq += nextWord;
	    prev[prev.length - 1] = nextWord;

	    // Get the predicted next word
	    nextWord = predictWord(prevSeq);

	    // Update the number of words generated
	    numWords++;
	}

	// ADd last word and return result
	sb.append(" " + nextWord);
	return sb.toString();
    }




    public static void main(String[] args) {
	NGram nGram = new NGram(3);

	if(args.length < 1) {
	    System.out.println("Please specify training file(s) from list:\n");
	    
	    File dir = new File(".");
	    File[] fileList = dir.listFiles();
	    for(File f: fileList) {
		String fileName = f.getName();
		if(fileName.endsWith(Formatter.TXT_EXT) 
		   && !fileName.contains(Formatter.FORMAT_TAG)) {
		    System.out.println(fileName);
		}
	    }

	    System.out.println();
	    return;
	}
	
	for(int fileIndex = 0; fileIndex < args.length; fileIndex++) {
	    String fileName = args[fileIndex];
	    Formatter f = new Formatter(fileName);
	    String train = f.format();
	    nGram.learn(train);
	}

	System.out.println("Generating text\n\n");

	// Generate 10 sentences according to training corpus style
	for(int sentCount = 0; sentCount < 10; sentCount++) {
	    System.out.println(nGram.generateText() + "\n");
	}
    }

}